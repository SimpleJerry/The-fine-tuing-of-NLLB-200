# The-fine-tuing-of-NLLB-200

nllb-fine-tune_20240616.ipynb：基于nllb-200-distilled-600M预训练模型的中译韩翻译模型的微调。数据集选用AI-Hub 한국어-중국어 번역 말뭉치(기술과학)中的자동차/교통/소재(30만)部分，其中有训练数据24万条和验证数据3万条（估计官方保留了3万条测试数据未予公开），我再从训练数据中划分出3万条作为训练数据，因此训练集、验证集、训练集的比例为21万:3万:3万。

nllb-fine-tune_20240618.ipynb：由于前一次实验中epoch设置得不够大，导致没能早停，所以增大了轮数继续训练，实验结果基本和第一次一样（甚至略低于）。并且修改了一些不合理的参数，例如validation step，之前的设置得过小，导致过于频繁的验证，拉长了训练耗时，现在改成了每1/5轮数验证一次



辅助脚本：

train_eval_loss_picture.ipynb：用于绘制训练集损失与验证集损失曲线

model-inference.ipynb：用于对模型进行推理（选用）
