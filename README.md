# The-fine-tuing-of-NLLB-200

nllb-fine-tune_20240616.ipynb：基于nllb-200-distilled-600M预训练模型的中译韩翻译模型的微调。数据集选用AI-Hub 한국어-중국어 번역 말뭉치(기술과학)中的자동차/교통/소재(30만)部分，其中有训练数据24万条和验证数据3万条（估计官方保留了3万条测试数据未予公开），我再从训练数据中划分出3万条作为训练数据，因此训练集、验证集、训练集的比例为21万:3万:3万。

nllb-fine-tune_20240618.ipynb：由于前一次实验中epoch设置得不够大，导致没能早停，所以增大了轮数继续训练，实验结果基本和第一次一样（甚至略低于）。并且修改了一些不合理的参数，例如validation step，之前的设置得过小，导致过于频繁的验证，拉长了训练耗时，现在改成了每1/5轮数验证一次

辅助脚本：

train_eval_loss_picture.ipynb：用于绘制训练集损失与验证集损失曲线

model-inference.ipynb：用于对模型进行推理（选用）

----------------------------------------------------------

nllb-fine-tune_20240616.ipynb: nllb-200-distilled-600M 사전 학습 모델을 기반으로 한 중문-한국어 번역 모델의 파인튜닝입니다. 데이터셋은 AI-Hub 한국어-중국어 번역 말뭉치(기술과학) 중 자동차/교통/소재(30만 개)를 선택하였으며, 이 중 훈련 데이터는 24만 개, 검증 데이터는 3만 개로 구성됨(3만 개의 테스트 데이터를 공개하지 않은 것으로 추정됨). 여기서 훈련 데이터에서 3만 개를 추가로 분리하여 검증 데이터로 사용하였기 때문에, 최종적으로 훈련 데이터, 검증 데이터, 테스트 데이터의 비율은 21만:3만:3만으로 설정됨.

nllb-fine-tune_20240618.ipynb: 이전 실험에서 epoch 설정을 충분히 크게 하지 않아 조기 종료가 이루어지지 않았으므로, epoch 수를 늘려 계속 훈련을 진행. 실험 결과는 기본적으로 첫 번째 실험과 비슷했으며(심지어 약간 낮은 결과도 나옴). 또한 validation step과 같은 일부 비합리적인 매개변수를 수정하였음. 이전에는 validation step이 너무 작게 설정되어 검증이 지나치게 빈번하게 이루어졌고, 이로 인해 훈련 시간이 길어졌음. 이를 개선하여 1/5 epoch 마다 검증을 수행하도록 수정함.

보조 스크립트:

train_eval_loss_picture.ipynb: 훈련 데이터 손실 및 검증 데이터 손실 곡선을 그리기 위한 스크립트.

model-inference.ipynb: 모델 추론을 수행하기 위한 스크립트(선택적으로 사용).
